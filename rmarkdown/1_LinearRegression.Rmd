# 1. Linear Regression

## Q 1
Let:
$$
Q(\beta_{0}, \beta_{1}) := \sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}.
$$

Taking the first order conditions, we obtain:
$$
\begin{aligned}
\begin{cases}
0 = \frac{\partial Q}{\partial \beta_{0}} = -2 \sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i}) \\
0 = \frac{\partial Q}{\partial \beta_{1}} = -2 \sum_{i=1}^{N} x_{i}(y_{i} - \beta_{0} - \beta_{1}x_{i})
\end{cases} \\
\therefore
\begin{cases}
\sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i}) = 0 \\
\sum_{i=1}^{N} x_{i}(y_{i} - \beta_{0} - \beta_{1}x_{i}) = 0
\end{cases}.
\end{aligned}
$$

From the first equation,
$$
\begin{aligned}
\sum_{i=1}^{N} y_{i} - N \beta_{0} - \beta_{1}\sum_{i=1}^{N} x_{i} &= 0 \\
\frac{1}{N}\sum_{i=1}^{N} y_{i} - \beta_{0} - \beta_{1}\frac{1}{N}\sum_{i=1}^{N} x_{i} &= 0 \\
\therefore \beta_{0} = \bar{y} - \beta_{1}\bar{x}.
\end{aligned}
$$

Insert this into the second equation in the first conditions,
$$
\begin{aligned}
&\sum_{i=1}^{N} x_{i}(y_{i} - \bar{y} + \beta_{1}\bar{x} - \beta_{1}x_{i}) = 0 \\
&\sum_{i=1}^{N} x_{i}y_{i}  - \bar{y}\sum_{i=1}^{N}x_{i} + \beta_{1}\bar{x}\sum_{i=1}^{N}x_{i} - \beta_{1}\sum_{i=1}^{N}x_{i}^{2} = 0 \\
&\left( \sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} \right)\beta_{1} = \sum_{i=1}^{N} x_{i}y_{i} - \bar{y}\sum_{i=1}^{N}x_{i}.
\end{aligned}
$$

Here,
$$
\begin{aligned}
\sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} &= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + \bar{x}\sum_{i=1}^{N}x_{i} \\
&= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + N \bar{x} \frac{1}{N} \sum_{i=1}^{N}x_{i} \\
&= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + N \bar{x}^{2} \\
&=  \sum_{i=1}^{N} (x_{i}^{2} - 2\bar{x}x_{i} + \bar{x}^{2}) \\
&=  \sum_{i=1}^{N} (x_{i} - \bar{x})^{2}. \\
\therefore \sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} &=  \sum_{i=1}^{N} (x_{i} - \bar{x})^{2}.
\end{aligned}
$$

Analogously, we can show that
$$
 \sum_{i=1}^{N} x_{i}y_{i} - \bar{y}\sum_{i=1}^{N}x_{i} = \sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y}).
$$

Therefore it follows that
$$
\begin{aligned}
\left(\sum_{i=1}^{N} (x_{i} - \bar{x})^{2} \right) \beta_{1} &= \sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y}) \\
\therefore \beta_{1} &= \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}
\end{aligned}
$$

Thus
$$
\hat{\beta_{0}} = \bar{y} - \beta_{1}\bar{x}, \quad \hat{\beta_{1}} = \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
$$

Since the function $Q$ is convex, $\left(\hat{\beta_{0}}, \hat{\beta_{1}} \right)$ is the minimizer of $Q$.


## Q 2
Let the intercept and the slope of $l^{\prime}$ be $\tilde{\beta_{0}}$ and $\tilde{\beta_{1}}$ respectively. The equation of $l^{\prime}$ is
$$
y = \tilde{\beta_{0}} + \tilde{\beta_{1}}x.
$$

Since $l$ passes through $(x_{i} - \bar{x}, y_{i} - \bar{y}) \quad (i = 1, \cdots, N)$,

$$
y_{i} - \bar{y} = \tilde{\beta_{0}} + \tilde{\beta_{1}}(x_{i} - \bar{x}) \quad (i = 1, \cdots, N).
$$

By summing up for $i = 1, \cdots, N$,
$$
\begin{aligned}
\sum_{i=1}^{N}y_{i} - N \bar{y} &= N \tilde{\beta_{0}} + \tilde{\beta_{1}} \left( \sum_{i=1}^{N}x_{i} -N \bar{x}\right) \\
\sum_{i=1}^{N}y_{i} - \sum_{i=1}^{N}y_{i} &= N \tilde{\beta_{0}} + \tilde{\beta_{1}} \left( \sum_{i=1}^{N}x_{i} -\sum_{i=1}^{N}x_{i}\right) \\
\therefore \tilde{\beta_{0}} = 0.
\end{aligned}
$$

Then we get
$$
y_{i} - \bar{y} = \tilde{\beta_{1}}(x_{i} - \bar{x}) \quad (i = 1, \cdots, N).
$$
By multiplying $(x_{i} - \bar{x})$ on both sides,
$$
(y_{i} - \bar{y})(x_{i} - \bar{x}) = \tilde{\beta_{1}}(x_{i} - \bar{x})^{2} \quad (i = 1, \cdots, N).
$$
Summing up for $i = 1, \cdots, N$,
$$
\begin{aligned}
\sum_{i=1}^{N}(y_{i} - \bar{y})(x_{i} - \bar{x}) &= \tilde{\beta_{1}} \left( \sum_{i=1}^{N}(x_{i} -\bar{x})^{2}\right) \\
\therefore \tilde{\beta_{1}} &= \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
\end{aligned}
$$

Thus

$$
\tilde{\beta_{0}} = 0, \quad \tilde{\beta_{1}} = \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
$$

After obtaining $\hat{\beta_{1}}$, we can get $\hat{\beta_{0}}$ from
$$
\hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}}\bar{x}.
$$

# Q 3

```{r}
# Set seed
set.seed(1)
# Number of observations
N <- 100
# Intercept and slope
a <- rnorm(1)
b <- rnorm(1)
# Data points
x <- rnorm(N)
y <- a * x + b + rnorm(N)
# Mean centering
x_center <- x - mean(x)
y_center <- y - mean(y)
# Dataframe
data_3 <- data.frame(group = "Original (l)", x, y) %>%
  dplyr::bind_rows(
    data.frame(group = "Centered (l')", x = x_center, y = y_center)
  ) %>%
  dplyr::mutate(
    group = forcats::fct_relevel(group, "Original (l)", "Centered (l')")
  )
# Graph
g_3 <- ggplot(data = data_3, aes(x = x, y = y, color = group)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, fullrange = TRUE) +
  geom_vline(xintercept = 0, linetype = "solid") +
  geom_hline(yintercept = 0, linetype = "solid")
# Plot the graph
plot(g_3)
```

# Q 4

## (a)

For any $x \in \mathbb{R}^{m}$,

$$
\begin{aligned}
x^{\top} A x &= x^{\top} B^{\top}B x \\
&= (Bx)^{\top}Bx \\
&= \left\Vert Bx \right\Vert^{2} \\
&\geq 0.
\end{aligned}
$$

Thus $A$ is positive semi-definite.

## (b)

Let $\Lambda := \text{diag}(\lambda_{1}, \cdots, \lambda_{m}) \in \mathbb{R}^{m \times m}$ and  $\sqrt{\Lambda} := \text{diag}(\sqrt{\lambda_{1}}, \cdots, \sqrt{\lambda_{m}}) \in \mathbb{R}^{m \times m}$. For any $x \in \mathbb{R}$,

$$
\begin{aligned}
x^{\top} A x &= x^{\top} P^{\top} \Lambda P x \\
&= y^{\top} \sqrt{\Lambda}^{\top} \sqrt{\Lambda} y \\
&=  \left\Vert \sqrt{\Lambda} y \right\Vert^{2} \\
&= \sum_{i=1}^{m} \sqrt{\lambda_{i}}^{2} y_{i}^{2} \\
&= \sum_{i=1}^{m} \lambda_{i} y_{i}^{2}.
\end{aligned}
$$

We will prove $\lambda_{i} \geq 0$ for any $i \in \{1, \cdots, m\}$ by proof by contradiction.

Suppose there exists $i \in \{1, \cdots, m\}$ such that $\lambda_{i} < 0$. Take $x = P^{-1}y$ such that $y_{i} = 1$ and $y_{j} = 0 \quad (j \neq i)$. Then
$$
\begin{aligned}
\left\Vert Bx \right\Vert^{2} &= x^{\top} B^{\top}B x \\
&= x^{\top} A x \\
&= \sum_{k=1}^{m} \lambda_{k} y_{k}^{2} \quad (\text{From the result above}) \\
&= \lambda_{i} \\
&< 0.
\end{aligned}
$$

However, this contraticts to the fact that $\left\Vert Bx \right\Vert^{2} \geq 0$, which completes the proof. 

## (c)

Take any $z \in \mathbb{R}^{m}$.

Suppose that $Az = 0$. Then

$$
\begin{aligned}
Az = 0 &\Leftrightarrow B^{\top}Bz = 0 \\
&\Rightarrow z^{\top}B^{\top}Bz = 0 \\
&\Rightarrow \left\Vert Bz \right\Vert^{2} = 0 \\
&\Leftrightarrow Bz = 0 \\
\therefore Az = 0 &\Rightarrow Bz = 0.
\end{aligned}
$$

Suppose that $Bz = 0$. Then

$$
\begin{aligned}
Bz = 0 &\Rightarrow B^{\top}Bz = 0 \\
&\Leftrightarrow Az = 0 \\
\therefore Bz = 0 &\Rightarrow Az = 0.
\end{aligned}
$$

Therefore
$$
Az = 0 \Leftrightarrow Bz = 0.
$$

## (d)

Suppose $A$ is non-singular. Then

$$
\begin{aligned}
m = \text{rank}(A) &= \text{rank}(B^{\top}B) \leq \text{rank}(B) = \min(m, n). \\
&\therefore m \leq \min(m, n). \\
&\therefore \min(m, n) = m. \\
&\therefore m \leq n.
\end{aligned}
$$

Then $\text{rank}(B) = m$.

Suppose $m \leq n$ and $\text{rank}(B) = m$. Then we have $\dim(\ker(B)) = 0$. Note that $\dim(\text{Im}(A)) + \dim(\ker(A)) = m$ and that $\dim(\ker(A)) = \dim(\ker(B)) = 0$ from (c). Then we have $\dim(\text{Im}(A)) = m$. Therefore, $A$ is non-singular.

Thus we obtain
$$
A \text{ is non-singular} \Leftrightarrow \text{rank}(B) = m \text{ and } m \leq n.
$$



# Q 12

```{r}
# Number of observations
N <- 100
# Generate data
x <- rnorm(N)
y <- rnorm(N)
# Compute mean
x_bar <- mean(x)
y_bar <- mean(y)
# Compute OLS
beta0 <- sum(y_bar * sum(x^2) - x_bar * sum(x * y)) / sum((x - x_bar)^2) 
beta1 <- sum((x - x_bar) * (y - y_bar)) / sum((x - x_bar)^2) 
# Compute residual sum of squares
RSS <- sum((y - beta0 - beta1 * x)^2)
RSE <- sqrt(RSS / (N - 1 - 1)) 
B0 <- (sum(x^2) / N) / sum((x - x_bar)^2)
B1 <- 1 / sum((x - x_bar)^2) 
# Standard errors
se0 <- RSE * sqrt(B0)
se1 <- RSE * sqrt(B1)
# t statistics
t0 <- beta0/se0
t1 <- beta1/se1
# p values
p0 <- 2 * (1 - pt(abs(t0), N - 2))
p1 <- 2 * (1 - pt(abs(t1),N - 2))
# Show the outputs
estimate_12 <- 
  data.frame(
    param = c('intercept', 'slope'),
    beta = c(beta0, beta1),
    se = c(se0, se1),
    t_stat = c(t0, t1),
    p_value = c(p0, p1)
  )
estimate_12
```

```{r}
summary(lm(y ~ x))
```



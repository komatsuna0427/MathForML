# 1. Linear Regression

## Q 1
Let:
$$
Q(\beta_{0}, \beta_{1}) := \sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i})^{2}.
$$

Taking the first order conditions, we obtain:
$$
\begin{aligned}
\begin{cases}
0 = \frac{\partial Q}{\partial \beta_{0}} = -2 \sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i}) \\
0 = \frac{\partial Q}{\partial \beta_{1}} = -2 \sum_{i=1}^{N} x_{i}(y_{i} - \beta_{0} - \beta_{1}x_{i})
\end{cases} \\
\therefore
\begin{cases}
\sum_{i=1}^{N} (y_{i} - \beta_{0} - \beta_{1}x_{i}) = 0 \\
\sum_{i=1}^{N} x_{i}(y_{i} - \beta_{0} - \beta_{1}x_{i}) = 0
\end{cases}.
\end{aligned}
$$

From the first equation,
$$
\begin{aligned}
\sum_{i=1}^{N} y_{i} - N \beta_{0} - \beta_{1}\sum_{i=1}^{N} x_{i} &= 0 \\
\frac{1}{N}\sum_{i=1}^{N} y_{i} - \beta_{0} - \beta_{1}\frac{1}{N}\sum_{i=1}^{N} x_{i} &= 0 \\
\therefore \beta_{0} = \bar{y} - \beta_{1}\bar{x}.
\end{aligned}
$$

Insert this into the second equation in the first conditions,
$$
\begin{aligned}
&\sum_{i=1}^{N} x_{i}(y_{i} - \bar{y} + \beta_{1}\bar{x} - \beta_{1}x_{i}) = 0 \\
&\sum_{i=1}^{N} x_{i}y_{i}  - \bar{y}\sum_{i=1}^{N}x_{i} + \beta_{1}\bar{x}\sum_{i=1}^{N}x_{i} - \beta_{1}\sum_{i=1}^{N}x_{i}^{2} = 0 \\
&\left( \sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} \right)\beta_{1} = \sum_{i=1}^{N} x_{i}y_{i} - \bar{y}\sum_{i=1}^{N}x_{i}.
\end{aligned}
$$

Here,
$$
\begin{aligned}
\sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} &= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + \bar{x}\sum_{i=1}^{N}x_{i} \\
&= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + N \bar{x} \frac{1}{N} \sum_{i=1}^{N}x_{i} \\
&= \sum_{i=1}^{N}x_{i}^{2} - 2\bar{x}\sum_{i=1}^{N}x_{i} + N \bar{x}^{2} \\
&=  \sum_{i=1}^{N} (x_{i}^{2} - 2\bar{x}x_{i} + \bar{x}^{2}) \\
&=  \sum_{i=1}^{N} (x_{i} - \bar{x})^{2}. \\
\therefore \sum_{i=1}^{N}x_{i}^{2} - \bar{x}\sum_{i=1}^{N}x_{i} &=  \sum_{i=1}^{N} (x_{i} - \bar{x})^{2}.
\end{aligned}
$$

Analogously, we can show that
$$
 \sum_{i=1}^{N} x_{i}y_{i} - \bar{y}\sum_{i=1}^{N}x_{i} = \sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y}).
$$

Therefore it follows that
$$
\begin{aligned}
\left(\sum_{i=1}^{N} (x_{i} - \bar{x})^{2} \right) \beta_{1} &= \sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y}) \\
\therefore \beta_{1} &= \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}
\end{aligned}
$$

Thus
$$
\hat{\beta_{0}} = \bar{y} - \beta_{1}\bar{x}, \quad \hat{\beta_{1}} = \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
$$

Since the function $Q$ is convex, $\left(\hat{\beta_{0}}, \hat{\beta_{1}} \right)$ is the minimizer of $Q$.


## Q 2
Let the intercept and the slope of $l^{\prime}$ be $\tilde{\beta_{0}}$ and $\tilde{\beta_{1}}$ respectively. The equation of $l^{\prime}$ is
$$
y = \tilde{\beta_{0}} + \tilde{\beta_{1}}x.
$$

Since $l$ passes through $(x_{i} - \bar{x}, y_{i} - \bar{y}) \quad (i = 1, \cdots, N)$,

$$
y_{i} - \bar{y} = \tilde{\beta_{0}} + \tilde{\beta_{1}}(x_{i} - \bar{x}) \quad (i = 1, \cdots, N).
$$

By summing up for $i = 1, \cdots, N$,
$$
\begin{aligned}
\sum_{i=1}^{N}y_{i} - N \bar{y} &= N \tilde{\beta_{0}} + \tilde{\beta_{1}} \left( \sum_{i=1}^{N}x_{i} -N \bar{x}\right) \\
\sum_{i=1}^{N}y_{i} - \sum_{i=1}^{N}y_{i} &= N \tilde{\beta_{0}} + \tilde{\beta_{1}} \left( \sum_{i=1}^{N}x_{i} -\sum_{i=1}^{N}x_{i}\right) \\
\therefore \tilde{\beta_{0}} = 0.
\end{aligned}
$$

Then we get
$$
y_{i} - \bar{y} = \tilde{\beta_{1}}(x_{i} - \bar{x}) \quad (i = 1, \cdots, N).
$$
By multiplying $(x_{i} - \bar{x})$ on both sides,
$$
(y_{i} - \bar{y})(x_{i} - \bar{x}) = \tilde{\beta_{1}}(x_{i} - \bar{x})^{2} \quad (i = 1, \cdots, N).
$$
Summing up for $i = 1, \cdots, N$,
$$
\begin{aligned}
\sum_{i=1}^{N}(y_{i} - \bar{y})(x_{i} - \bar{x}) &= \tilde{\beta_{1}} \left( \sum_{i=1}^{N}(x_{i} -\bar{x})^{2}\right) \\
\therefore \tilde{\beta_{1}} &= \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
\end{aligned}
$$

Thus

$$
\tilde{\beta_{0}} = 0, \quad \tilde{\beta_{1}} = \frac{\sum_{i=1}^{N} (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum_{i=1}^{N} (x_{i} - \bar{x})^{2}}.
$$

After obtaining $\hat{\beta_{1}}$, we can get $\hat{\beta_{0}}$ from
$$
\hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}}\bar{x}.
$$

## Q 3

```{r}
# Set seed
set.seed(1)
# Number of observations
N <- 100
# Intercept and slope
a <- rnorm(1)
b <- rnorm(1)
# Data points
x <- rnorm(N)
y <- a * x + b + rnorm(N)
# Mean centering
x_center <- x - mean(x)
y_center <- y - mean(y)
# Dataframe
data_3 <- data.frame(group = "Original (l)", x, y) %>%
  dplyr::bind_rows(
    data.frame(group = "Centered (l')", x = x_center, y = y_center)
  ) %>%
  dplyr::mutate(
    group = forcats::fct_relevel(group, "Original (l)", "Centered (l')")
  )
# Graph
g_3 <- ggplot(data = data_3, aes(x = x, y = y, color = group)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, fullrange = TRUE) +
  geom_vline(xintercept = 0, linetype = "solid") +
  geom_hline(yintercept = 0, linetype = "solid")
# Plot the graph
plot(g_3)
```

## Q 4

### (a)

For any $x \in \mathbb{R}^{m}$,

$$
\begin{aligned}
x^{\top} A x &= x^{\top} B^{\top}B x \\
&= (Bx)^{\top}Bx \\
&= \left\Vert Bx \right\Vert^{2} \\
&\geq 0.
\end{aligned}
$$

Thus $A$ is positive semi-definite.

### (b)

Let $\Lambda := \text{diag}(\lambda_{1}, \cdots, \lambda_{m}) \in \mathbb{R}^{m \times m}$ and  $\sqrt{\Lambda} := \text{diag}(\sqrt{\lambda_{1}}, \cdots, \sqrt{\lambda_{m}}) \in \mathbb{R}^{m \times m}$. For any $x \in \mathbb{R}$,

$$
\begin{aligned}
x^{\top} A x &= x^{\top} P^{\top} \Lambda P x \\
&= y^{\top} \sqrt{\Lambda}^{\top} \sqrt{\Lambda} y \\
&=  \left\Vert \sqrt{\Lambda} y \right\Vert^{2} \\
&= \sum_{i=1}^{m} \sqrt{\lambda_{i}}^{2} y_{i}^{2} \\
&= \sum_{i=1}^{m} \lambda_{i} y_{i}^{2}.
\end{aligned}
$$

We will prove $\lambda_{i} \geq 0$ for any $i \in \{1, \cdots, m\}$ by proof by contradiction.

Suppose there exists $i \in \{1, \cdots, m\}$ such that $\lambda_{i} < 0$. Take $x = P^{-1}y$ such that $y_{i} = 1$ and $y_{j} = 0 \quad (j \neq i)$. Then
$$
\begin{aligned}
\left\Vert Bx \right\Vert^{2} &= x^{\top} B^{\top}B x \\
&= x^{\top} A x \\
&= \sum_{k=1}^{m} \lambda_{k} y_{k}^{2} \quad (\text{From the result above}) \\
&= \lambda_{i} \\
&< 0.
\end{aligned}
$$

However, this contraticts to the fact that $\left\Vert Bx \right\Vert^{2} \geq 0$, which completes the proof. 

### (c)

Take any $z \in \mathbb{R}^{m}$.

Suppose that $Az = 0$. Then

$$
\begin{aligned}
Az = 0 &\Leftrightarrow B^{\top}Bz = 0 \\
&\Rightarrow z^{\top}B^{\top}Bz = 0 \\
&\Rightarrow \left\Vert Bz \right\Vert^{2} = 0 \\
&\Leftrightarrow Bz = 0 \\
\therefore Az = 0 &\Rightarrow Bz = 0.
\end{aligned}
$$

Suppose that $Bz = 0$. Then

$$
\begin{aligned}
Bz = 0 &\Rightarrow B^{\top}Bz = 0 \\
&\Leftrightarrow Az = 0 \\
\therefore Bz = 0 &\Rightarrow Az = 0.
\end{aligned}
$$

Therefore
$$
Az = 0 \Leftrightarrow Bz = 0.
$$

### (d)

Suppose $A$ is non-singular. Then

$$
\begin{aligned}
m = \text{rank}(A) &= \text{rank}(B^{\top}B) \leq \text{rank}(B) = \min(m, n). \\
&\therefore m \leq \min(m, n). \\
&\therefore \min(m, n) = m. \\
&\therefore m \leq n.
\end{aligned}
$$

Then $\text{rank}(B) = m$.

Suppose $m \leq n$ and $\text{rank}(B) = m$. Then we have $\dim(\ker(B)) = 0$. Note that $\dim(\text{Im}(A)) + \dim(\ker(A)) = m$ and that $\dim(\ker(A)) = \dim(\ker(B)) = 0$ from (c). Then we have $\dim(\text{Im}(A)) = m$. Therefore, $A$ is non-singular.

Thus we obtain
$$
A \text{ is non-singular} \Leftrightarrow \text{rank}(B) = m \text{ and } m \leq n.
$$


## Q 5

### (a) When $N < p + 1$:

We prove this by proof by contradiction. Suppose that $X^{\top}X$ is invertible. Then $\text{rank}(X^{\top}X) = p+1$. From the result from Q 4,

$$
\begin{aligned}
p + 1 = \text{rank}(X^{\top}X)  \leq \text{rank}(X) &= \min(n, p+1) = N \quad (\text{since } N < p+1) \\
\therefore p + 1 &\leq N,
\end{aligned}
$$
which is a contradiction.

### (b) When $N \geq p + 1$ and there are two identical columns in $X$:

Since $X$ is not of full column rank, $\text{rank}(X) < p+1$. Then

$$
\begin{aligned}
p+1 > \text{rank}(X) &\geq \text{rank}(X^{\top}X) \\
\therefore \text{rank}(X^{\top}X) < p+1
\end{aligned}
$$

Therefore, $X^{\top}X$ is not invertible.


## Q 7

### (a)

$$
\begin{aligned}
\hat{\beta} &= (X^{\top}X)^{-1}X^{\top}y \\
&= (X^{\top}X)^{-1}X^{\top} (X\beta + \epsilon) \\
&= (X^{\top}X)^{-1}(X^{\top}X)\beta + (X^{\top}X)^{-1}X^{\top}\epsilon \\
&= \beta + (X^{\top}X)^{-1}X^{\top}\epsilon.
\end{aligned}
$$

### (b)

What we need to show is $\mathbb{E}(\hat{\beta}) = \beta$.

$$
\begin{aligned}
\mathbb{E}(\hat{\beta}) &= \mathbb{E}(\beta + (X^{\top}X)^{-1}X^{\top}\epsilon) \\
&= \beta + \mathbb{E}((X^{\top}X)^{-1}X^{\top}\epsilon) \\
&= \beta + \mathbb{E}[\mathbb{E} \{(X^{\top}X)^{-1}X^{\top}\epsilon \vert  X\}] \quad (\text{by the law of iterated expectation}) \\
&= \beta + \mathbb{E}[ (X^{\top}X)^{-1}X^{\top}\mathbb{E}\{\epsilon \vert  X\}] \\
&= \beta + \mathbb{E}[ (X^{\top}X)^{-1}X^{\top}\mathbb{E}\{\epsilon\}] \quad (\text{since } X \text{ and } \epsilon \text{ are independent}) \\
&= \beta \quad (\mathbb{E}(\epsilon) = 0).
\end{aligned}
$$

### (c)

$$
\begin{aligned}
(\hat{\beta} - \beta)(\hat{\beta} - \beta)^{\top} &= (X^{\top}X)^{-1}X^{\top}\epsilon [(X^{\top}X)^{-1}X^{\top}\epsilon]^{\top} \\
&= (X^{\top}X)^{-1}X^{\top}\epsilon \epsilon^{\top} X (X^{\top}X)^{-1}
\end{aligned}
$$

Since $\sigma^{2}I = \mathbb{E}[(\epsilon - \mathbb{E}(\epsilon)(\epsilon - \mathbb{E}(\epsilon)^{\top}] = \mathbb{E}[\epsilon \epsilon^{\top}]$, and $X$ and $\epsilon$ are independent, it follows that

$$
\begin{aligned}
\mathbb{E}[\epsilon \epsilon^{\top} \vert X] &= \mathbb{E}[\epsilon \epsilon^{\top}] = \sigma^{2}I.
\end{aligned}
$$

Then

$$
\begin{aligned}
\mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^{\top} \vert X] &= 
\mathbb{E}[(X^{\top}X)^{-1}X^{\top}\epsilon \epsilon^{\top} X (X^{\top}X)^{-1} \vert X] \\
&= (X^{\top}X)^{-1}X^{\top} \underbrace{\mathbb{E}[\epsilon \epsilon^{\top} \vert X]}_{=\sigma^{2}I} X (X^{\top}X)^{-1} \\
&= \sigma^{2} (X^{\top}X)^{-1} X^{\top}X (X^{\top}X)^{-1} \\
&= \sigma^{2} (X^{\top}X)^{-1}
\end{aligned}
$$

## Q 8

### (a)

$$
\begin{aligned}
H^{2} 
&= X\underbrace{(X^{\top}X)^{-1}X^{\top}X}_{=I}(X^{\top}X)^{-1}X^{\top} \\
&= X(X^{\top}X)^{-1}X^{\top} \\
&= H.
\end{aligned}
$$


### (b)

$$
\begin{aligned}
(I-H)^{2} 
&= (I-H)(I-H) \\
&= I - 2H + H^{2} \\
&= I - 2H + H \\
&= I - H.
\end{aligned}
$$

### (c)

$$
\begin{aligned}
HX 
&= X\underbrace{(X^{\top}X)^{-1}X^{\top}X}_{=I} \\
&= X.
\end{aligned}
$$

### (d)

$$
\begin{aligned}
\hat{y}
:=& X\hat{\beta} \\
=& \underbrace{X(X^{\top}X)^{-1}X^{\top}}_{=:H}y \\
=& Hy.
\end{aligned}
$$

### (e)

$$
\begin{aligned}
y - \hat{y}
:=& y - Hy \\
=& (I-H)y \\
=& (I-H)(X\beta + \epsilon) \\
=& X\beta - \underbrace{HX}_{=X}\beta + (I-H)\epsilon \\
=& X\beta - X\beta + (I-H)\epsilon \\
=& (I-H)\epsilon.
\end{aligned}
$$

### (f)

Note that
$$
\begin{aligned}
(I-H)^{\top} 
=& I - H^{\top} \\
=& I - [X(X^{\top}X)X^{\top}]^{\top} \\
=& I - X[(X^{\top}X)^{-1}]^{\top}X^{\top} \\
=& I - X(X^{\top}X)^{-1}X^{\top} \\
=& I - H.
\end{aligned}
$$

Then

$$
\begin{aligned}
\left \Vert y - \hat{y} \right \Vert 
=& (y - \hat{y})^{\top}(y - \hat{y}) \\
=& [(I-H)\epsilon]^{\top}(I-H)\epsilon \\
=& \epsilon^{\top}(I-H)^{\top}(I-H)\epsilon \\
=& \epsilon^{\top}(I-H)^{2}\epsilon \\
=& \epsilon^{\top}(I-H)\epsilon.
\end{aligned}
$$



## Q 12

```{r}
# Number of observations
N <- 100
# Generate data
x <- rnorm(N)
y <- rnorm(N)
# Compute mean
x_bar <- mean(x)
y_bar <- mean(y)
# Compute OLS
beta0 <- sum(y_bar * sum(x^2) - x_bar * sum(x * y)) / sum((x - x_bar)^2) 
beta1 <- sum((x - x_bar) * (y - y_bar)) / sum((x - x_bar)^2) 
# Compute residual sum of squares
RSS <- sum((y - beta0 - beta1 * x)^2)
RSE <- sqrt(RSS / (N - 1 - 1)) 
B0 <- (sum(x^2) / N) / sum((x - x_bar)^2)
B1 <- 1 / sum((x - x_bar)^2) 
# Standard errors
se0 <- RSE * sqrt(B0)
se1 <- RSE * sqrt(B1)
# t statistics
t0 <- beta0/se0
t1 <- beta1/se1
# p values
p0 <- 2 * (1 - pt(abs(t0), N - 2))
p1 <- 2 * (1 - pt(abs(t1),N - 2))
# Show the outputs
estimate_12 <- 
  data.frame(
    param = c('intercept', 'slope'),
    beta = c(beta0, beta1),
    se = c(se0, se1),
    t_stat = c(t0, t1),
    p_value = c(p0, p1)
  )
estimate_12
```

```{r}
summary(lm(y ~ x))
```


